{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T22:30:34.430068Z",
     "start_time": "2024-12-13T22:30:34.425314Z"
    }
   },
   "source": [
    "# 1) quasi random numbers\n",
    "# 2) importance sampling: sample from the region of the tail (look for extremes) that is most important, this will reduce variance "
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "BASELINE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70602e955cd0fc57"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np, pandas as pd, scipy.stats as st"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:41:29.450388800Z",
     "start_time": "2024-12-14T00:41:27.919882800Z"
    }
   },
   "id": "7676e8236601dd5e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "num_obvs = 30_000\n",
    "# risks per sector\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "#sec_loading maps sector to its risk, t is the threshold for defaults\n",
    "data['sec_loading'], data['t'] = r[data['sector'].values], st.norm.ppf(data.p)\n",
    "# 100k monte carlo simulations and len(r)+len(data) risk factors per one sample\n",
    "factors, sample, vars = np.random.normal(0,1, (num_obvs, len(r)+len(data))), [], []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:41:33.861039400Z",
     "start_time": "2024-12-14T00:41:29.450388800Z"
    }
   },
   "id": "a283bacc6b9730e0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "for obs in factors:\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "    ind = r[0]**.5 * m_factor + (data.sec_loading-r[0])**.5 * sec_factor + (1-\n",
    "                                                                            data.sec_loading)**.5 * res_factor < data.t\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * np.random.standard_t(3, size=sum(ind\n",
    "                                                                             ))\n",
    "    sample.append(sum(loss))\n",
    "    vars.append(np.var(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:42:44.550922900Z",
     "start_time": "2024-12-14T00:41:33.865038Z"
    }
   },
   "id": "fff02203cb4f61ad",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# extracts the 1% quantile from the sorted list of 100k obsercations\n",
    "VaR = sorted([-s for s in sample])[int(0.001*num_obvs)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:45:12.410379900Z",
     "start_time": "2024-12-14T00:45:12.268374900Z"
    }
   },
   "id": "d70f892eb407ee96",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1322.440973848404"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = np.mean(vars)\n",
    "var"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:42:44.575431800Z",
     "start_time": "2024-12-14T00:42:44.566131400Z"
    }
   },
   "id": "7f1f1f3c2ca9a926",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-18580.024077247774"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:45:15.345783600Z",
     "start_time": "2024-12-14T00:45:15.315338700Z"
    }
   },
   "id": "8ebff0913b7c6f97",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#SMALLEST VALUE PREDICTED BY MONTE CARLO\n",
    "np.min(sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e02de411f29768e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VaR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1897101f07dfe04c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "IMPORTANCE SELECTION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1653b8a18b00561"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sample, bins=50, density=True)\n",
    "plt.title(\"Preliminary Loss Distribution\")\n",
    "plt.xlabel(\"Total Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(-40000,-500)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2306f7feefbb67d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def target_pdf(x):\n",
    "    return st.t.pdf(x, df=3)\n",
    "\n",
    "def proposal_pdf(x, shift=-15000):\n",
    "    return st.t.pdf(x - shift, df=3)\n",
    "\n",
    "def generate_proposal_samples(size, shift=-10000):\n",
    "    return st.t.rvs(df=3, loc=shift, size=size)\n",
    "\n",
    "def importance_sampling_var(num_samples=100_000, shift=-10000):\n",
    "    sample_losses = []\n",
    "    weights = []\n",
    "\n",
    "    proposal_samples = generate_proposal_samples(num_samples, shift)\n",
    "\n",
    "    for obs in proposal_samples:\n",
    "        w = target_pdf(obs) / proposal_pdf(obs, shift)\n",
    "\n",
    "        m_factor = np.random.normal(0, 1)  # Global market factor\n",
    "        sec_factor = np.random.normal(0, 1, len(r))  # Sector factors\n",
    "        res_factor = obs  # Residual factor from the proposal distribution\n",
    "\n",
    "        ind = (r[0]**0.5 * m_factor +\n",
    "               (data.sec_loading - r[0])**0.5 * sec_factor[data['sector'].values] +\n",
    "               (1 - data.sec_loading)**0.5 * res_factor) < data['t']\n",
    "\n",
    "        loss = np.zeros(len(data))\n",
    "        loss[ind] = data['m'][ind] + data['d'][ind] * res_factor\n",
    "\n",
    "        # Weighted loss\n",
    "        sample_losses.append(np.sum(loss) * w)\n",
    "        weights.append(w)\n",
    "\n",
    "    sorted_losses = np.array(sample_losses)[np.argsort(sample_losses)]\n",
    "    sorted_weights = np.array(weights)[np.argsort(sample_losses)]\n",
    "    cumulative_weights = np.cumsum(sorted_weights) / np.sum(sorted_weights)\n",
    "\n",
    "    VaR_index = np.searchsorted(cumulative_weights, 0.999)\n",
    "    VaR = sorted_losses[VaR_index]\n",
    "\n",
    "    return VaR, sample_losses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "718d78c66fd322f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Var2, losses2 = importance_sampling_var()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87a5da33301062e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7867dc3ba9fcfc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "VERSION 2 IMPORTANCE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96f957feb11ae9f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data/data.csv')\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "data['sec_loading'], data['t'] = r[data['sector'].values], st.norm.ppf(data.p)\n",
    "\n",
    "# Importance Sampling Parameters\n",
    "shift = 2.0  # Shift parameter to bias the sampling distribution towards the tail\n",
    "\n",
    "# Simulate factors and initialize\n",
    "factors = np.random.normal(shift, 1, (100_000, len(r) + len(data)))  # Shifted Gaussian\n",
    "sample, weights = [], []\n",
    "\n",
    "for obs in factors:\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "\n",
    "    # Adjusted loss indicator threshold\n",
    "    ind = np.sqrt(r[0]) * m_factor + np.sqrt(data.sec_loading - r[0]) * sec_factor \\\n",
    "          + np.sqrt(1 - data.sec_loading) * res_factor < (data.t - shift)\n",
    "\n",
    "    # Compute losses\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * np.random.standard_t(3, size=sum(ind))\n",
    "\n",
    "    # Compute weight for importance sampling correction\n",
    "    weight = np.exp(-shift * obs[0] + 0.5 * shift**2)\n",
    "    weights.append(weight)\n",
    "    sample.append(sum(loss) * weight)  # Adjusted loss weighted\n",
    "\n",
    "# Calculate VaR using weighted losses\n",
    "weighted_losses = -np.array(sample)\n",
    "sorted_losses = np.sort(weighted_losses)\n",
    "VaR = sorted_losses[int(0.999 * len(sorted_losses))]  # Empirical 99.9% quantile\n",
    "print(f\"VaR with Importance Sampling: {VaR}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65eecbfbf1ddcd4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "HEREZJA SOBOL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95bb7337b7c4c5f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_dimensions = len(r) + len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "724adddd0e5a941a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "sobol_sampler = qmc.Sobol(d=num_dimensions, scramble=True)\n",
    "quasi_random_samples = sobol_sampler.random_base2(m=int(np.log2(num_obvs)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba399899dffab16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "factors = st.norm.ppf(quasi_random_samples)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d29e71e93c630d33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "sample = []\n",
    "sobol_student= qmc.Sobol(d=1, scramble=True)\n",
    "for obs in factors:\n",
    "    # m_factor, sec_factor, res_factor = obs[0], obs[1:1 + len(r)][data.sector.values], obs[len(r):]\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "\n",
    "    ind = (\n",
    "                  r[0]**0.5 * m_factor\n",
    "                  + (data.sec_loading - r[0])**0.5 * sec_factor\n",
    "                  + (1 - data.sec_loading)**0.5 * res_factor\n",
    "          ) < data.t\n",
    "    loss = np.zeros((len(data),))\n",
    "    stt=sobol_student.random(n=sum(ind)).flatten()\n",
    "    stud = st.t.ppf(stt,3)\n",
    "    loss[ind] = data[ind].m + data[ind].d * stud\n",
    "    sample.append(sum(loss))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cde17e16a69583e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VaR = sorted([-s for s in sample])[int(0.01 * num_obvs)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3688278ff925b476"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VaR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3faec3b8639e8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PARALLEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf17e452f3711197"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scipy.stats as st\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "num_obvs = 1000000\n",
    "# risks per sector\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "#sec_loading maps sector to its risk, t is the threshold for defaults\n",
    "data['sec_loading'], data['t'] = r[data['sector'].values], st.norm.ppf(data.p)\n",
    "# 100k monte carlo simulations and len(r)+len(data) risk factors per one sample\n",
    "num_dimensions = len(r) + len(data)\n",
    "from scipy.stats import qmc\n",
    "\n",
    "sobol_sampler = qmc.Sobol(d=num_dimensions, scramble=True)\n",
    "quasi_random_samples = sobol_sampler.random_base2(m=int(np.log2(num_obvs)))\n",
    "from scipy.stats import norm\n",
    "\n",
    "factors = st.norm.ppf(quasi_random_samples)\n",
    "\n",
    "\n",
    "def process_obs(obs):\n",
    "    # m_factor, sec_factor, res_factor = obs[0], obs[1:1 + len(r)][data.sector.values], obs[len(r):]\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "\n",
    "    ind = (\n",
    "                  r[0] ** 0.5 * m_factor\n",
    "                  + (data.sec_loading - r[0]) ** 0.5 * sec_factor\n",
    "                  + (1 - data.sec_loading) ** 0.5 * res_factor\n",
    "          ) < data.t\n",
    "    loss = np.zeros((len(data),))\n",
    "    stt = sobol_student.random(n=sum(ind)).flatten()\n",
    "    stud = st.t.ppf(stt, 3)\n",
    "    loss[ind] = data[ind].m + data[ind].d * stud\n",
    "    return sum(loss)\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from scipy.stats import t\n",
    "\n",
    "sample = []\n",
    "sobol_student = qmc.Sobol(d=1, scramble=True)\n",
    "\n",
    "sample = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in factors)\n",
    "VaR = sorted([-s for s in sample])[int(0.01 * num_obvs)]\n",
    "VaR\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3535ac419ef464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T22:30:59.159824Z",
     "start_time": "2024-12-13T22:30:59.158572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factors = np.random.normal(0, 1, (num_obvs // 2, len(r) + len(data)))\n",
    "antithetic_factors = -factors\n",
    "\n",
    "factors = np.vstack([factors, antithetic_factors])\n",
    "sample_antithetic = []\n",
    "\n",
    "for obs in factors:\n",
    "    m_factor = obs[0] \n",
    "    sec_factor = obs[:len(r)][data.sector.values] \n",
    "    res_factor = obs[len(r):]  \n",
    "\n",
    "    ind = (\n",
    "                  r[0]**0.5 * m_factor\n",
    "                  + (data.sec_loading - r[0])**0.5 * sec_factor\n",
    "                  + (1 - data.sec_loading)**0.5 * res_factor\n",
    "          ) < data.t\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * t.rvs(df=3,size=sum(ind))\n",
    "    sample.append(sum(loss))\n",
    "    \n",
    "    sample_antithetic.append(sum(loss))\n",
    "    \n",
    "    Var_antithetic = sorted([-s for s in sample])[100]"
   ],
   "id": "35485171e6f3b46e",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T22:31:59.259683Z",
     "start_time": "2024-12-13T22:31:59.257540Z"
    }
   },
   "cell_type": "code",
   "source": "Var_antithetic",
   "id": "2b0756734cabd965",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10697.589807499246"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
