{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scipy.stats as st\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "num_obvs = 30_000\n",
    "# risks per sector\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "#sec_loading maps sector to its risk, t is the threshold for defaults\n",
    "data['sec_loading'], data['t'] = r[data['sector'].values], st.norm.ppf(data.p)\n",
    "# 100k monte carlo simulations and len(r)+len(data) risk factors per one sample\n",
    "num_dimensions = len(r) + len(data)\n",
    "from scipy.stats import qmc\n",
    "\n",
    "sobol_sampler = qmc.Sobol(d=num_dimensions, scramble=True)\n",
    "quasi_random_samples = sobol_sampler.random_base2(m=int(np.log2(num_obvs)))\n",
    "from scipy.stats import norm\n",
    "\n",
    "#factors = st.norm.ppf(quasi_random_samples)\n",
    "shift = 2.0  # Shift parameter to bias the sampling distribution towards the tail\n",
    "\n",
    "# Simulate factors and initialize\n",
    "factors = np.random.normal(shift, 1, (100_000, len(r) + len(data)))  # Shifted Gaussian\n",
    "\n",
    "def process_obs(obs):\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "\n",
    "    # Adjusted loss indicator threshold\n",
    "    ind = np.sqrt(r[0]) * m_factor + np.sqrt(data.sec_loading - r[0]) * sec_factor \\\n",
    "          + np.sqrt(1 - data.sec_loading) * res_factor < (data.t - shift)\n",
    "\n",
    "    # Compute losses\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * np.random.standard_t(3, size=sum(ind))\n",
    "\n",
    "    # Compute weight for importance sampling correction\n",
    "    weight = np.exp(-shift * obs[0] + 0.5 * shift**2)\n",
    "    return np.sum(loss) * weight, weight\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from scipy.stats import t\n",
    "\n",
    "sample = []\n",
    "sobol_student = qmc.Sobol(d=1, scramble=True)\n",
    "\n",
    "answers = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in factors)\n",
    "answers = np.array(answers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbbf5c403435865d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "samples = answers[:, 0]\n",
    "print(len(samples))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3a075045bc093e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weights = answers[:, 1]\n",
    "print(len(weights))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafca56f6826c3cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate VaR using weighted losses\n",
    "weighted_losses = -np.array(samples)\n",
    "sorted_losses = np.sort(weighted_losses)\n",
    "VaR = sorted_losses[int(0.999 * len(sorted_losses))]  # Empirical 99.9% quantile\n",
    "print(f\"VaR with Importance Sampling: {VaR}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c989e1f2b96a3c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sorted_samples = np.argsort(samples)\n",
    "sorted_weights = np.argsort(weights)\n",
    "cumulative_weights = np.cumsum(sorted_samples) / np.sum(sorted_weights)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3131703c57d8df3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VaR_index = np.searchsorted(cumulative_weights, 0.0001)\n",
    "VaR = sorted_samples[VaR_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a66f2c22a1261e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sorted_samples[:100]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51bb160c2d623488",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VaR = sorted([-s for s in sample])[int(0.01 * num_obvs)]\n",
    "VaR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d285e6a73abdade7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15527848f4f0a6a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "14437a7c84c8953a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANTH"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e45b3f4b8b2f06a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scipy.stats as st\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "num_obvs = 30_000\n",
    "# risks per sector\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "#sec_loading maps sector to its risk, t is the threshold for defaults\n",
    "sec_loading, datat = r[data['sector'].values], st.norm.ppf(data.p)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:19.857691800Z",
     "start_time": "2024-12-14T02:43:19.816831600Z"
    }
   },
   "id": "a8e1239ac901de5a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "factors, sample = np.random.normal(0,1, (num_obvs, len(r)+len(data))), []\n",
    "\n",
    "antithetic_factors = -factors\n",
    "\n",
    "factors = np.vstack([factors, antithetic_factors])\n",
    "sample_antithetic = []\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][data.sector.values]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    ind = (\n",
    "                  r[0]**0.5 * m_factor\n",
    "                  + (sec_loading - r[0])**0.5 * sec_factor\n",
    "                  + (1 - sec_loading)**0.5 * res_factor\n",
    "          ) < datat\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * t.rvs(df=3,size=sum(ind))\n",
    "    return sum(loss), np.var(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:26.359285400Z",
     "start_time": "2024-12-14T02:43:19.971816400Z"
    }
   },
   "id": "651f09eeece532a3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from scipy.stats import t\n",
    "\n",
    "answers = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in factors)\n",
    "answers = np.array(answers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:43.195852100Z",
     "start_time": "2024-12-14T02:43:26.361284600Z"
    }
   },
   "id": "3c80775e1b38f89c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-21825.159480973656"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_antithetic = answers[:, 0]\n",
    "vars = answers[:, 1]\n",
    "Var_antithetic = sorted([-s for s in sample_antithetic])[int(0.001*num_obvs)]\n",
    "Var_antithetic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:43.242712600Z",
     "start_time": "2024-12-14T02:43:43.196852700Z"
    }
   },
   "id": "8a4c5bb44d6ea202",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1342.5501061476914"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:43.244712300Z",
     "start_time": "2024-12-14T02:43:43.228168Z"
    }
   },
   "id": "d75b0a7f0dd5a303",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:43.257680300Z",
     "start_time": "2024-12-14T02:43:43.233663300Z"
    }
   },
   "id": "71bf17c6ecfe7a7d",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "NEW"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecaf2eab4fbc47a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Parametry do stratyfikacji\n",
    "num_strata = 10\n",
    "strata_bounds = np.linspace(-3, 3, num_strata + 1)\n",
    "\n",
    "# Funkcja generująca stratyfikowane czynniki\n",
    "def stratified_factors(num_samples, dim, strata_bounds):\n",
    "    factors = []\n",
    "    samples_per_stratum = num_samples // len(strata_bounds)\n",
    "    for i in range(len(strata_bounds) - 1):\n",
    "        low, high = strata_bounds[i], strata_bounds[i + 1]\n",
    "        stratum = np.random.uniform(low, high, (samples_per_stratum, dim))\n",
    "        factors.append(stratum)\n",
    "    return np.vstack(factors)\n",
    "\n",
    "# Funkcja obliczająca straty\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]  # Czynnik ogólny\n",
    "    sec_factor = obs[:len(r)][sector_indices]  # Czynniki sektorowe\n",
    "    res_factor = obs[len(r):]  # Czynniki resztowe\n",
    "\n",
    "    # Zmienna kontrolna: przybliżenie straty jako wartość oczekiwana\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Ograniczanie wartości t-Studenta, aby zmniejszyć wpływ ekstremalnych strat\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros((len(data),))\n",
    "    if np.any(ind):\n",
    "        loss[ind] = data.loc[ind, 'm'].values + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "\n",
    "    # Korekcja zmienną kontrolną\n",
    "    E_control_variate = 0  # Oczekiwana wartość dla zmiennej kontrolnej\n",
    "    c = np.cov(loss, control_variate)[0, 1] / np.var(control_variate) if np.var(control_variate) > 0 else 0\n",
    "    corrected_loss = loss - c * (control_variate - E_control_variate)\n",
    "    return np.sum(corrected_loss), np.var(corrected_loss)\n",
    "\n",
    "# Generowanie stratyfikowanych próbek\n",
    "factors = stratified_factors(num_obvs, len(r) + len(data), strata_bounds)\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Obliczenia w równoległych procesach\n",
    "answers = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in combined_factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_antithetic = answers[:, 0]\n",
    "vars = answers[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:58.287875300Z",
     "start_time": "2024-12-14T02:43:43.238679300Z"
    }
   },
   "id": "cfe84310fa0b2ad",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji: 58876.901880159356\n",
      "VaR antetyczny (100-ty wynik): 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Symetryzacja wyników i redukcja wariancji\n",
    "Var_antithetic = sorted(np.abs(sample_antithetic))[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyniki końcowe\n",
    "print(\"Średnia wariancji:\", np.mean(vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", Var_antithetic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:43:58.303619600Z",
     "start_time": "2024-12-14T02:43:58.289842Z"
    }
   },
   "id": "34bae3ea58131777",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Parametry do stratyfikacji\n",
    "num_strata = 10\n",
    "strata_bounds = np.linspace(-3, 3, num_strata + 1)\n",
    "\n",
    "# Funkcja generująca stratyfikowane czynniki\n",
    "def stratified_factors(num_samples, dim, strata_bounds):\n",
    "    factors = []\n",
    "    samples_per_stratum = num_samples // len(strata_bounds)\n",
    "    for i in range(len(strata_bounds) - 1):\n",
    "        low, high = strata_bounds[i], strata_bounds[i + 1]\n",
    "        stratum = np.random.uniform(low, high, (samples_per_stratum, dim))\n",
    "        factors.append(stratum)\n",
    "    return np.vstack(factors)\n",
    "\n",
    "# Funkcja obliczająca straty\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]  # Czynnik ogólny\n",
    "    sec_factor = obs[:len(r)][sector_indices]  # Czynniki sektorowe\n",
    "    res_factor = obs[len(r):]  # Czynniki resztowe\n",
    "\n",
    "    # Zmienna kontrolna: przybliżenie straty jako wartość oczekiwana\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Ograniczanie wartości t-Studenta, aby zmniejszyć wpływ ekstremalnych strat\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros((len(data),))\n",
    "    if np.any(ind):\n",
    "        loss[ind] = data.loc[ind, 'm'].values + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "\n",
    "    # Korekcja zmienną kontrolną\n",
    "    E_control_variate = 0  # Oczekiwana wartość dla zmiennej kontrolnej\n",
    "    c = np.cov(loss, control_variate)[0, 1] / np.var(control_variate) if np.var(control_variate) > 0 else 0\n",
    "    corrected_loss = loss - c * (control_variate - E_control_variate)\n",
    "    return np.sum(corrected_loss), np.var(corrected_loss)\n",
    "\n",
    "# Generowanie stratyfikowanych próbek\n",
    "factors = stratified_factors(num_obvs, len(r) + len(data), strata_bounds)\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Obliczenia w równoległych procesach\n",
    "answers = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in combined_factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_antithetic = answers[:, 0]\n",
    "vars = answers[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:15.214899900Z",
     "start_time": "2024-12-14T02:43:58.306590300Z"
    }
   },
   "id": "49d079dc990fc90",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 44128 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed:   12.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 1361.5702719671224\n",
      "VaR antetyczny (100-ty wynik): -26383.838126145893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   12.6s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty dla jednej obserwacji\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]  # Czynnik ogólny\n",
    "    sec_factor = obs[:len(r)][sector_indices]  # Czynniki sektorowe\n",
    "    res_factor = obs[len(r):]  # Czynniki resztowe\n",
    "\n",
    "    # Obliczanie zmiennej kontrolnej dla indywidualnych strat\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Indeks dla obserwacji poniżej progu\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    # Ograniczenie wartości z rozkładu t-Studenta\n",
    "    if np.any(ind):\n",
    "        loss[ind] = data.loc[ind, 'm'].values + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "\n",
    "    # Zwróć sumę strat i wariancję strat\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek antetycznych\n",
    "factors = np.random.normal(0, 1, (num_obvs, len(r) + len(data)))\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "answers = Parallel(n_jobs=-1, verbose=5)(delayed(process_obs)(obs) for obs in combined_factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR dla antetycznych próbek\n",
    "VaR_antithetic = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyniki końcowe\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", VaR_antithetic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:32.903197100Z",
     "start_time": "2024-12-14T02:44:15.217933400Z"
    }
   },
   "id": "4bdd1f9a8b35eefe",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses = np.array(sample_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:32.910129700Z",
     "start_time": "2024-12-14T02:44:32.905196200Z"
    }
   },
   "id": "d9c4b25485448e95",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-43573.52068710552"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_99_9= np.percentile(losses, 0.001)\n",
    "extreme_losses = losses[losses<quantile_99_9]\n",
    "np.median(extreme_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:32.923012600Z",
     "start_time": "2024-12-14T02:44:32.910129700Z"
    }
   },
   "id": "aba37624de2ecb3b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:32.924012200Z",
     "start_time": "2024-12-14T02:44:32.917014900Z"
    }
   },
   "id": "92f0fa780540af75",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    7.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 1349.8830658967622\n",
      "VaR antetyczny (100-ty wynik): -24649.74392752423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 32768 out of 32768 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm, qmc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty dla jednej obserwacji\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]  # Czynnik ogólny\n",
    "    sec_factor = obs[:len(r)][sector_indices]  # Czynniki sektorowe\n",
    "    res_factor = obs[len(r):]  # Czynniki resztowe\n",
    "\n",
    "    # Obliczanie zmiennej kontrolnej dla indywidualnych strat\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Indeks dla obserwacji poniżej progu\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    # Ograniczenie wartości z rozkładu t-Studenta\n",
    "    if np.any(ind):\n",
    "        loss[ind] = data.loc[ind, 'm'].values + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "\n",
    "    # Zwróć sumę strat i wariancję strat\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek za pomocą sekwencji Sobola\n",
    "def generate_sobol_factors(num_samples, dim):\n",
    "    sampler = qmc.Sobol(d=dim, scramble=True)\n",
    "    sobol_samples = sampler.random_base2(m=int(np.log2(num_samples)))\n",
    "\n",
    "    # Konwersja próbek Sobola na rozkład normalny\n",
    "    sobol_factors = norm.ppf(sobol_samples)\n",
    "    return sobol_factors\n",
    "\n",
    "# Generowanie quasi-Monte Carlo próbek\n",
    "sobol_factors = generate_sobol_factors(num_obvs, len(r) + len(data))\n",
    "\n",
    "# Generowanie antetycznych próbek\n",
    "antithetic_factors = -sobol_factors\n",
    "combined_factors = np.vstack([sobol_factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "answers = Parallel(n_jobs=-1, verbose=5)(delayed(process_obs)(obs) for obs in combined_factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR dla antetycznych próbek\n",
    "VaR_antithetic = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyniki końcowe\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", VaR_antithetic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:44:49.565036Z",
     "start_time": "2024-12-14T02:44:32.925013900Z"
    }
   },
   "id": "51fc5738e17a4666",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 44128 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed:   12.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 1335.481869102711\n",
      "VaR antetyczny (100-ty wynik): -25925.59041319705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   12.7s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Wartość oczekiwana dla uproszczonej zmiennej kontrolnej\n",
    "expected_loss = np.mean(data['m'])\n",
    "\n",
    "# Funkcja obliczająca straty z użyciem zmiennych kontrolnych\n",
    "def process_obs_with_cv(obs):\n",
    "    m_factor = obs[0]  # Czynnik ogólny\n",
    "    sec_factor = obs[:len(r)][sector_indices]  # Czynniki sektorowe\n",
    "    res_factor = obs[len(r):]  # Czynniki resztowe\n",
    "\n",
    "    # Obliczanie zmiennej kontrolnej\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Indeks dla obserwacji poniżej progu\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    # Obliczenie strat i zmiennej kontrolnej\n",
    "    if np.any(ind):\n",
    "        loss[ind] = data.loc[ind, 'm'].values + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "    raw_loss = np.sum(loss)\n",
    "\n",
    "    # Dodanie zmiennej kontrolnej (wzór redukujący wariancję)\n",
    "    cv_adjustment = (np.sum(control_variate) / len(control_variate)) - expected_loss\n",
    "    adjusted_loss = raw_loss - cv_adjustment\n",
    "\n",
    "    # Zwróć stratę i jej wariancję\n",
    "    return adjusted_loss, np.var(loss)\n",
    "\n",
    "# Generowanie próbek Monte Carlo z antetycznymi próbkami\n",
    "factors = np.random.normal(0, 1, (num_obvs, len(r) + len(data)))\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "answers = Parallel(n_jobs=-1, verbose=5)(delayed(process_obs_with_cv)(obs) for obs in combined_factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_antithetic = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyniki końcowe\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", VaR_antithetic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:45:07.311983200Z",
     "start_time": "2024-12-14T02:44:49.583035400Z"
    }
   },
   "id": "dfa27b0672e12b9a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:45:07.317988400Z",
     "start_time": "2024-12-14T02:45:07.312554Z"
    }
   },
   "id": "5193bb3790b1a786",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie obliczeń...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0091s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0676s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0631s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0799s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1090s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 816 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1420s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1920 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4640 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7712 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 9312 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 11040 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 14624 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 18464 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 20448 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 22560 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 24672 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 26912 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 31520 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 33888 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 36384 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 38880 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 41504 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 44128 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 46880 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 49632 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 52512 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 55392 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 58400 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 59684 tasks      | elapsed:   12.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 1362.583623111245\n",
      "VaR antetyczny (100-ty wynik): -26211.276589941175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   12.8s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Liczba obserwacji\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty dla jednej próby\n",
    "def process_obs(obs):\n",
    "    # Wybór czynników (ogólny, sektorowy, resztowy)\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][sector_indices]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    # Obliczenie indywidualnej zmiennej losowej\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "    # Sprawdzenie progu\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    # Generowanie strat dla spełnionych warunków\n",
    "    if np.any(ind):\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie antetycznych próbek\n",
    "dim = len(r) + len(data)\n",
    "factors = np.random.normal(0, 1, (num_obvs, dim))\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "print(\"Rozpoczęcie obliczeń...\")\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs)(obs) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_antithetic = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", VaR_antithetic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:45:25.279576200Z",
     "start_time": "2024-12-14T02:45:07.321992Z"
    }
   },
   "id": "bacda75488051823",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap iteracja 1/5...\n",
      "Bootstrap iteracja 2/5...\n",
      "Bootstrap iteracja 3/5...\n",
      "Bootstrap iteracja 4/5...\n",
      "Bootstrap iteracja 5/5...\n",
      "Średnia wariancji strat: 1355.7348658528772\n",
      "VaR antetyczny (100-ty wynik): -8304.806069612436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 15_000  # Zmniejszona liczba prób\n",
    "num_bootstraps = 5  # Liczba powtórzeń bootstrap\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty dla jednej próby\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][sector_indices]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie antetycznych próbek\n",
    "def generate_factors(num_obvs, dim):\n",
    "    factors = np.random.normal(0, 1, (num_obvs, dim))\n",
    "    antithetic_factors = -factors\n",
    "    return np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Funkcja bootstrapująca\n",
    "def bootstrap_run():\n",
    "    combined_factors = generate_factors(num_obvs, len(r) + len(data))\n",
    "    answers = Parallel(n_jobs=-1)(\n",
    "        delayed(process_obs)(obs) for obs in combined_factors\n",
    "    )\n",
    "    answers = np.array(answers)\n",
    "    return answers[:, 0], answers[:, 1]\n",
    "\n",
    "# Główna pętla bootstrap\n",
    "all_losses, all_vars = [], []\n",
    "for i in range(num_bootstraps):\n",
    "    print(f\"Bootstrap iteracja {i+1}/{num_bootstraps}...\")\n",
    "    losses, vars_ = bootstrap_run()\n",
    "    all_losses.append(losses)\n",
    "    all_vars.append(vars_)\n",
    "\n",
    "# Agregowanie wyników\n",
    "all_losses = np.vstack(all_losses)\n",
    "all_vars = np.vstack(all_vars)\n",
    "\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "mean_vars = np.mean(all_vars, axis=0)\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_antithetic = sorted(mean_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(mean_vars))\n",
    "print(\"VaR antetyczny (100-ty wynik):\", VaR_antithetic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:10.081742300Z",
     "start_time": "2024-12-14T02:45:25.279576200Z"
    }
   },
   "id": "74d0c093c65ed167",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:10.086917400Z",
     "start_time": "2024-12-14T02:46:10.083378200Z"
    }
   },
   "id": "24463f7a4d75cac7",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie Importance Sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0212s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0589s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0660s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0750s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0975s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 816 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0961s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1920 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4640 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7712 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 9312 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 11040 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 14624 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 18464 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 20448 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 22560 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 24672 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 26912 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 31520 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 33888 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 36384 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 38880 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 41504 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 44128 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 46880 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 49632 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 52512 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 55392 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 58400 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 59684 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   12.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 94.53512777298837\n",
      "VaR z Importance Sampling (100-ty wynik): inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja Importance Sampling\n",
    "def process_obs_IS(obs, bias):\n",
    "    # Dodanie biasu do próbek\n",
    "    obs_shifted = obs + bias\n",
    "\n",
    "    # Wybór czynników (ogólny, sektorowy, resztowy)\n",
    "    m_factor = obs_shifted[0]\n",
    "    sec_factor = obs_shifted[:len(r)][sector_indices]\n",
    "    res_factor = obs_shifted[len(r):]\n",
    "\n",
    "    # Obliczenie zmiennej losowej i strat\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    # Korekcja wag (Importance Sampling)\n",
    "    weight = np.exp(-0.5 * np.sum(bias**2) + 0.5 * np.sum(obs**2))\n",
    "    weighted_loss = np.sum(loss) * weight\n",
    "\n",
    "    return weighted_loss, np.var(loss)\n",
    "\n",
    "# Generowanie próbek z Importance Sampling\n",
    "dim = len(r) + len(data)\n",
    "bias = np.ones(dim) * 0.5  # Wektor biasu przesuwający rozkład\n",
    "\n",
    "factors = np.random.normal(0, 1, (num_obvs, dim))\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "print(\"Rozpoczęcie Importance Sampling...\")\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_IS)(obs, bias) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_IS = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR z Importance Sampling (100-ty wynik):\", VaR_IS)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:27.789664Z",
     "start_time": "2024-12-14T02:46:10.095918800Z"
    }
   },
   "id": "e03e3d8099777712",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie obliczeń z nowym rozkładem t-Studenta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0080s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0630s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0719s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0967s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1550s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 816 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1670s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1920 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4640 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7712 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 9312 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 11040 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 14624 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 16480 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 18464 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 20448 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 22560 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 24672 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 26912 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 29152 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 31520 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 33888 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 36384 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 38880 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 41504 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 44128 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 46880 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 49632 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 52512 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 55392 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 58400 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 59684 tasks      | elapsed:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 1355.052253839234\n",
      "VaR z t-Studenta (100-ty wynik): -27808.47321504282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   13.3s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Zwiększona liczba prób\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty z mniejszymi stopniami swobody (więcej ciężkich ogonów)\n",
    "def process_obs_stable(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][sector_indices]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    # Obliczenie zmiennej losowej z t-Studenta\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Zmiana wartości granicznych\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        # Zastosowanie rozkładu t-Studenta z mniejszymi stopniami swobody dla większej odpornosci na ekstremalne straty\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=4, size=sum(ind)), -10, 10)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek i antetycznych próbek\n",
    "dim = len(r) + len(data)\n",
    "factors = np.random.normal(0, 1, (num_obvs, dim))\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "print(\"Rozpoczęcie obliczeń z nowym rozkładem t-Studenta...\")\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_stable)(obs) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_stable = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR z t-Studenta (100-ty wynik):\", VaR_stable)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:46.085892500Z",
     "start_time": "2024-12-14T02:46:27.793680400Z"
    }
   },
   "id": "acc4c878a618d0f4",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0250s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0010s.) Setting batch_size=4.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5658,) (5666,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\giantuss\\AppData\\Local\\Temp\\ipykernel_15968\\3201993324.py\", line 20, in process_obs_weighted\nValueError: operands could not be broadcast together with shapes (5658,) (5666,) \n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 60\u001B[0m\n\u001B[0;32m     57\u001B[0m combined_factors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([factors, antithetic_factors])\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# Równoległe przetwarzanie próbek\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m answers \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)(\n\u001B[0;32m     61\u001B[0m     delayed(process_obs_weighted)(obs, weight_factor)\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obs, weight_factor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(combined_factors, weights)\n\u001B[0;32m     63\u001B[0m )\n\u001B[0;32m     64\u001B[0m answers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(answers)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# Wyniki\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve()\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout))\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[0;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m future\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5658,) (5666,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Liczba prób Monte Carlo\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty z ważeniem\n",
    "def process_obs_weighted(obs, weight_factor):\n",
    "    # Modyfikacja rozkładu próbek przez skalowanie wagami\n",
    "    m_factor = obs[0] * weight_factor\n",
    "    sec_factor = obs[:len(r)][sector_indices] * weight_factor\n",
    "    res_factor = obs[len(r):] * weight_factor\n",
    "\n",
    "    # Obliczenie zmiennej losowej\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Graniczny próg\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        # Zastosowanie rozkładu t-Studenta dla resztowych strat\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Funkcja dla obliczania wag (ważenie obszarów)\n",
    "def calculate_weights(factors):\n",
    "    # Proste ważenie bazujące na rozkładzie normalnym, które koncentruje się na dużych stratach\n",
    "    weights = np.exp(-np.abs(factors) / np.std(factors))\n",
    "    return weights\n",
    "\n",
    "# Generowanie próbek\n",
    "dim = len(r) + len(data)\n",
    "factors = np.random.normal(0, 1, (num_obvs, dim))\n",
    "\n",
    "# Obliczanie wag dla próbek\n",
    "weights = calculate_weights(factors)\n",
    "\n",
    "# Generowanie próbek antytetycznych\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_weighted)(obs, weight_factor)\n",
    "    for obs, weight_factor in zip(combined_factors, weights)\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_weighted = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR ważony (100-ty wynik):\", VaR_weighted)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:56.810682400Z",
     "start_time": "2024-12-14T02:46:46.097893900Z"
    }
   },
   "id": "6269ba69abb5a904",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    3.6s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5658,) (29992,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\giantuss\\AppData\\Local\\Temp\\ipykernel_15968\\1007999817.py\", line 38, in process_obs_antithetic\nValueError: operands could not be broadcast together with shapes (5658,) (29992,) \n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 62\u001B[0m\n\u001B[0;32m     59\u001B[0m combined_factors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([stratified_factors, antithetic_factors])\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Równoległe przetwarzanie próbek\u001B[39;00m\n\u001B[1;32m---> 62\u001B[0m answers \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)(\n\u001B[0;32m     63\u001B[0m     delayed(process_obs_antithetic)(obs) \u001B[38;5;28;01mfor\u001B[39;00m obs \u001B[38;5;129;01min\u001B[39;00m combined_factors\n\u001B[0;32m     64\u001B[0m )\n\u001B[0;32m     65\u001B[0m answers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(answers)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# Wyniki\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve()\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout))\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[0;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m future\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5658,) (29992,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Liczba prób Monte Carlo\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja do podziału próbek na warstwy\n",
    "def stratified_sampling(num_obvs, num_strata):\n",
    "    # Tworzenie warstw: w tym przypadku losowe podzielenie próbek\n",
    "    strata = np.linspace(-4, 4, num_strata)  # Rozkład warstw w przestrzeni\n",
    "    samples = []\n",
    "\n",
    "    for i in range(num_strata):\n",
    "        strata_samples = np.random.normal(strata[i], 1, num_obvs // num_strata)\n",
    "        samples.append(strata_samples)\n",
    "\n",
    "    return np.concatenate(samples)\n",
    "\n",
    "# Funkcja obliczająca straty z antytezą\n",
    "def process_obs_antithetic(obs):\n",
    "    # Użycie próbek antytetycznych\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][sector_indices]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Graniczny próg\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        # Zastosowanie rozkładu t-Studenta dla resztowych strat\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek z stratified sampling\n",
    "num_strata = 10  # Liczba warstw, można dostosować\n",
    "stratified_factors = stratified_sampling(num_obvs, num_strata)\n",
    "\n",
    "# Generowanie próbek antytetycznych\n",
    "antithetic_factors = -stratified_factors\n",
    "combined_factors = np.vstack([stratified_factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_antithetic)(obs) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_stratified = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR stratified (100-ty wynik):\", VaR_stratified)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:53:27.670146400Z",
     "start_time": "2024-12-14T02:53:23.624361800Z"
    }
   },
   "id": "8a0531687ff0e21e",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_qmc.py:804: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie obliczeń z metodą Quasi-Monte Carlo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1894s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0650s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0770s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0910s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1030s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1410s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1344 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2080 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1710s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3680 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6432 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 9888 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 13344 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 17056 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 20768 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 24736 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 28704 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 32928 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 37152 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 41632 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 46112 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 50848 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 55584 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 59326 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 59568 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 59691 tasks      | elapsed:   16.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia wariancji strat: 0.0\n",
      "VaR z QMC (100-ty wynik): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 59814 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed:   16.9s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Zwiększona liczba prób\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values\n",
    "sec_loading = r[sector_indices]\n",
    "datat = norm.ppf(data['p'])\n",
    "\n",
    "# Funkcja obliczająca straty\n",
    "def process_obs_qmc(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][sector_indices]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    # Obliczenie zmiennej losowej\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Indeksy strat\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    if np.any(ind):\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek z użyciem punktów Sobola\n",
    "sampler = qmc.Sobol(d=len(r) + len(data), scramble=True)\n",
    "factors = sampler.random(num_obvs)  # Generujemy próbki Sobola\n",
    "factors = 2 * factors - 1  # Transformacja na rozkład jednostajny [-1, 1]\n",
    "\n",
    "# Zastosowanie antetycznych próbek\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "print(\"Rozpoczęcie obliczeń z metodą Quasi-Monte Carlo...\")\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_qmc)(obs) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_qmc = sorted(sample_losses)[int(0.001*num_obvs)]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR z QMC (100-ty wynik):\", VaR_qmc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:53:50.866259900Z",
     "start_time": "2024-12-14T02:53:29.963703600Z"
    }
   },
   "id": "d69b801d5cd78e6e",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_qmc.py:804: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  sample = self._random(n, workers=workers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczęcie obliczeń z metodą Quasi-Monte Carlo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1090s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0070s.) Setting batch_size=4.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5658,) (8,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\giantuss\\AppData\\Local\\Temp\\ipykernel_15968\\34539293.py\", line 26, in process_obs_qmc\nValueError: operands could not be broadcast together with shapes (5658,) (8,) \n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 53\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Równoległe przetwarzanie próbek\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRozpoczęcie obliczeń z metodą Quasi-Monte Carlo...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 53\u001B[0m answers \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)(\n\u001B[0;32m     54\u001B[0m     delayed(process_obs_qmc)(obs) \u001B[38;5;28;01mfor\u001B[39;00m obs \u001B[38;5;129;01min\u001B[39;00m combined_factors\n\u001B[0;32m     55\u001B[0m )\n\u001B[0;32m     56\u001B[0m answers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(answers)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Wyniki\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve()\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout))\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[0;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m future\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5658,) (8,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Parametry symulacji\n",
    "num_obvs = 30_000  # Zwiększona liczba prób\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "sector_indices = data['sector'].values  # Indeksy sektorów w danych\n",
    "sec_loading = r[sector_indices]  # Ładowanie ryzyka po sektorach\n",
    "datat = norm.ppf(data['p'])  # Zmienna odpowiadająca wartościom prawdopodobieństwa\n",
    "\n",
    "# Funkcja obliczająca straty\n",
    "def process_obs_qmc(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[1:len(r)+1]  # Wartości dla każdego sektora\n",
    "    res_factor = obs[len(r)+1:]  # Reszta czynników\n",
    "\n",
    "    # Obliczenie zmiennej losowej\n",
    "    control_variate = (\n",
    "            r[0]**0.5 * m_factor\n",
    "            + (sec_loading - r[0])**0.5 * sec_factor\n",
    "            + (1 - sec_loading)**0.5 * res_factor\n",
    "    )\n",
    "\n",
    "    # Indeksy strat\n",
    "    ind = control_variate < datat\n",
    "    loss = np.zeros(len(data))\n",
    "\n",
    "    # Jeśli są jakieś straty (ind != 0)\n",
    "    if np.any(ind):\n",
    "        loss[ind] = (\n",
    "                data.loc[ind, 'm'].values\n",
    "                + data.loc[ind, 'd'].values * np.clip(t.rvs(df=3, size=sum(ind)), -5, 5)\n",
    "        )\n",
    "    return np.sum(loss), np.var(loss)\n",
    "\n",
    "# Generowanie próbek z użyciem punktów Sobola\n",
    "sampler = qmc.Sobol(d=len(r) + len(data), scramble=True)\n",
    "factors = sampler.random(num_obvs)  # Generujemy próbki Sobola\n",
    "factors = 2 * factors - 1  # Transformacja na rozkład jednostajny [-1, 1]\n",
    "\n",
    "# Zastosowanie antetycznych próbek\n",
    "antithetic_factors = -factors\n",
    "combined_factors = np.vstack([factors, antithetic_factors])\n",
    "\n",
    "# Równoległe przetwarzanie próbek\n",
    "print(\"Rozpoczęcie obliczeń z metodą Quasi-Monte Carlo...\")\n",
    "answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_obs_qmc)(obs) for obs in combined_factors\n",
    ")\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_qmc = sorted(sample_losses)[int(0.001*num_obvs)]  # Wyciąganie 100-tego wyniku z posortowanych strat\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Średnia wariancji strat:\", np.mean(sample_vars))\n",
    "print(\"VaR z QMC (100-ty wynik):\", VaR_qmc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:54:05.613649800Z",
     "start_time": "2024-12-14T02:53:54.408760500Z"
    }
   },
   "id": "c6d730fb11af29da",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "BASELINE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d10cdb27226577a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scipy.stats as st\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import qmc\n",
    "\n",
    "num_obvs = 30_000\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "data[\"sec_loading\"], data[\"t\"] = r[data[\"sector\"].values], st.norm.ppf(data.p)\n",
    "factors, sample = np.random.normal(0,1, (num_obvs, len(r)+len(data))), []\n",
    "\n",
    "def process_obs(obs):\n",
    "    m_factor, sec_factor, res_factor = obs[0], obs[:len(r)][data.sector.values], obs[len(r):]\n",
    "    ind = (\n",
    "            r[0]**.5 * m_factor + (data.sec_loading-r[0])**.5 * sec_factor + (1-data.sec_loading)**.5 * res_factor\n",
    "            < data.t\n",
    "    )\n",
    "    loss = np.zeros((len(data),))\n",
    "    loss[ind] = data[ind].m + data[ind].d * np.random.standard_t(3, size=sum(ind))\n",
    "    return sum(loss), np.var(loss)\n",
    "\n",
    "answers = Parallel(n_jobs=-1)(delayed(process_obs)(obs) for obs in factors)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# Wyniki\n",
    "sample_losses = answers[:, 0]\n",
    "sample_vars = answers[:, 1]\n",
    "\n",
    "# Obliczenie VaR\n",
    "VaR_qmc = sorted([-s for s in sample_losses])[int(0.001*num_obvs)]  # Wyciąganie 100-tego wyniku z posortowanych strat\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:54:26.779830400Z",
     "start_time": "2024-12-14T02:54:08.890185300Z"
    }
   },
   "id": "26fcd6fd4da9a46",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-17227.451262458308"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR_qmc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:54:26.788831800Z",
     "start_time": "2024-12-14T02:54:26.781831600Z"
    }
   },
   "id": "66c2e9305b1b2dd1",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1368.5773664035662"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sample_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:54:26.805344500Z",
     "start_time": "2024-12-14T02:54:26.786832700Z"
    }
   },
   "id": "8779da036ce45f06",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1885s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0670s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0851s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1240s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1490s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1840 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 3376 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4176 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 5040 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5904 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6832 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 7760 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 8752 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 9744 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 10800 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 11856 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 12976 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 14096 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 15280 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 16464 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 17712 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 18960 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 20272 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 21584 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 22960 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 24336 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 25776 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 27216 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 28720 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 30224 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 31792 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 33360 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 34992 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 36624 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 38320 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 40016 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 41776 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 43536 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 45360 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 47184 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 49072 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 50960 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 52912 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 54864 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 56880 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 58896 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 60976 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 63056 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 65200 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 67344 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 69552 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 71760 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 74032 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 76304 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 78640 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 80976 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 83376 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 85776 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 88240 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 90704 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 93232 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 95760 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 98352 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 99859 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100000 out of 100000 | elapsed:   38.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17624.812468017866, 1353.577483202275)]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import qmc, norm\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from scipy.stats import t\n",
    "\n",
    "num_obvs = 30_000\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "r = np.array([.295, .49, .41, .415, .338, .64, .403, .476])\n",
    "data[\"sec_loading\"], data[\"t\"] = r[data[\"sector\"].values], st.norm.ppf(data.p)\n",
    "factors, sample = np.random.normal(0,1, (num_obvs, len(r)+len(data))), []\n",
    "\n",
    "# Function to process a single observation\n",
    "def process_obs(obs):\n",
    "    m_factor = obs[0]\n",
    "    sec_factor = obs[:len(r)][data.sector.values]\n",
    "    res_factor = obs[len(r):]\n",
    "\n",
    "    ind = (\n",
    "                  r[0]**0.5 * m_factor\n",
    "                  + (data.sec_loading - r[0])**0.5 * sec_factor\n",
    "                  + (1 - data.sec_loading)**0.5 * res_factor\n",
    "          ) < data.t\n",
    "\n",
    "    loss = np.zeros(len(data))\n",
    "    loss[ind] = data[ind].m + data[ind].d * t.rvs(df=3, size=sum(ind))\n",
    "    return sum(loss), np.var(loss)\n",
    "\n",
    "# Generate LHS factors\n",
    "def generate_lhs_factors(num_obvs, num_dimensions):\n",
    "    # Create Latin Hypercube Sampler\n",
    "    lhs_sampler = qmc.LatinHypercube(d=num_dimensions)\n",
    "\n",
    "    # Generate LHS samples in [0, 1]\n",
    "    lhs_samples = lhs_sampler.random(n=num_obvs)\n",
    "\n",
    "    # Transform to standard normal distribution\n",
    "    lhs_factors = norm.ppf(lhs_samples)\n",
    "\n",
    "    return lhs_factors\n",
    "\n",
    "# Run LHS method\n",
    "def run_lhs(data, r, num_obvs=30_000):\n",
    "    num_dimensions = len(r) + len(data)  # Total risk factors\n",
    "    lhs_factors = generate_lhs_factors(num_obvs, num_dimensions)  # Generate LHS samples\n",
    "\n",
    "    # Parallelized loss computation\n",
    "    answers = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_obs)(obs) for obs in lhs_factors\n",
    "    )\n",
    "    answers = np.array(answers)\n",
    "    \n",
    "    sample = answers[:, 0]\n",
    "    vars = answers[:, 1]\n",
    "\n",
    "    # Compute VaR (99.9% quantile)\n",
    "    VaR = np.percentile(np.array(sample), 99.9)\n",
    "    var = np.mean(vars)\n",
    "    \n",
    "    return VaR, var\n",
    "\n",
    "# Simulation\n",
    "num_sims = [100000]\n",
    "lhs_results = [run_lhs(data, r, num_obvs=n) for n in num_sims]\n",
    "\n",
    "# Output results\n",
    "print(lhs_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:57:08.571518600Z",
     "start_time": "2024-12-14T02:54:26.793340300Z"
    }
   },
   "id": "68b6df09822d7e0d",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:54:05.657686200Z",
     "start_time": "2024-12-14T02:54:05.620656300Z"
    }
   },
   "id": "ff20016ec00b84a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a432eae0a601f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
